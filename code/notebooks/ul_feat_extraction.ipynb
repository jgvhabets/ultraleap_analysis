{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to load raw ultraleap data, \n",
    "save cleaned dataframes for each block, \n",
    "and generate dataframes of distances for further feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import public packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from itertools import compress\n",
    "\n",
    "\n",
    "import openpyxl\n",
    "from datetime import datetime\n",
    "import math\n",
    "import statistics as stat\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_path_in_notebook():\n",
    "    \"\"\"\n",
    "    Finds path of repo from Notebook.\n",
    "    Start running this once to correctly find\n",
    "    other modules/functions\n",
    "    \"\"\"\n",
    "    path = os.getcwd()\n",
    "    repo_name = 'ultraleap_analysis'\n",
    "\n",
    "    while path[-len(repo_name):] != 'ultraleap_analysis':\n",
    "\n",
    "        path = os.path.dirname(path)\n",
    "\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = get_repo_path_in_notebook()\n",
    "code_path = os.path.join(repo_path, 'code')\n",
    "os.chdir(code_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import own functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_data.import_and_convert_data as import_dat\n",
    "import import_data.find_paths as find_paths\n",
    "import import_data.preprocessing_meta_info as meta_info\n",
    "import sig_processing.segment_tasks as seg_tasks\n",
    "import movement_calc.helpfunctions as hp\n",
    "import feature_extraction.get_features as get_feat\n",
    "import feature_extraction.get_files as get_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading blocks for feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading own functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'feature_extraction.get_files' from 'c:\\\\Users\\\\habetsj\\\\Research\\\\projects\\\\ultraleap_analysis\\\\code\\\\feature_extraction\\\\get_files.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(import_dat)\n",
    "importlib.reload(seg_tasks)\n",
    "importlib.reload(find_paths)\n",
    "importlib.reload(meta_info)\n",
    "importlib.reload(hp)\n",
    "importlib.reload(get_feat)\n",
    "importlib.reload(get_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'patientdata'\n",
    "conds = ['m1', 'm0s0', 'm0s1', 'm1s0', 'm1s1']\n",
    "cams = ['dt', 'vr',  'st']\n",
    "tasks = ['ft', 'oc', 'ps']\n",
    "sides = ['left', 'right']\n",
    "subs = find_paths.find_available_subs(folder)\n",
    "subs.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving features per block as json files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in subs:\n",
    "    for task in tasks:\n",
    "        try:\n",
    "            files = os.listdir(os.path.join(repo_path, 'data','distances', folder, sub, task))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            continue \n",
    "        \n",
    "        for file in files:\n",
    "\n",
    "            if file == '.DS_Store':\n",
    "                continue\n",
    "\n",
    "            # Load blocks from patients' blocks dir\n",
    "            block = pd.read_csv(os.path.join(\n",
    "                repo_path, 'data', 'distances', folder, sub, task, file), index_col= 0)\n",
    "\n",
    "            block_features = get_feat.features_across_block(block, task)\n",
    "\n",
    "            feat_path = os.path.join(repo_path, 'data', 'features', 'feat_dict', folder, sub, task)\n",
    "            if not os.path.exists(feat_path): os.makedirs(feat_path)\n",
    "                    \n",
    "            get_files.savedict_as_json(feat_path, f'{file}', block_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving feature blocks per task as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another idea for csv saving function. Maybe revisit at later stage!\n",
    "# the function below assumes all the data is placed in one folder and nothing else is placed in this folder. \n",
    "# Adjust to do something like \"for i in subject_ids: with open(os.path.join(path, \"shows_\", i, \".json\")) as f:\"\n",
    "\n",
    "def create_feat_df_per_task(path = str):\n",
    "    data_all = pd.DataFrame()\n",
    "    for file in os.listdir(path):\n",
    "        with open(os.path.join(path, file)) as f:\n",
    "            data = json.load(f)\n",
    "            df = pd.DataFrame(data.values(), index=data.keys())\n",
    "            df = df.T\n",
    "            data_all = data_all.append(df, ignore_index = True)\n",
    "    return data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_val = []\n",
    "ft_val = []\n",
    "oc_val = []\n",
    "for task in tasks:\n",
    "    for sub in subs:\n",
    "        try:\n",
    "            files = os.listdir(os.path.join(repo_path, 'data','features', 'feat_dict', folder, sub, task))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            continue \n",
    "\n",
    "        for file in files:\n",
    "            # Create new dictionary with filename as first key (used for first column in df later)\n",
    "            feat_json = {'filename': f'{file}'}\n",
    "\n",
    "            # Load dictionary with bock features\n",
    "            feat_path = os.path.join(repo_path, 'data', 'features', 'feat_dict', folder, sub, task, file)\n",
    "            old_feat_json = get_files.loadjson_as_dict(feat_path)\n",
    "\n",
    "            # Update the new dictionary with block features \n",
    "            feat_json |= old_feat_json\n",
    "\n",
    "            # Make a list of dicts to later create a df based on task\n",
    "            if task == 'ft':\n",
    "                ft_val.append(feat_json)\n",
    "            elif task == 'oc':\n",
    "                oc_val.append(feat_json)\n",
    "            elif task == 'ps':\n",
    "                ps_val.append(feat_json)\n",
    "\n",
    "        ft_feat_df = pd.DataFrame(ft_val)\n",
    "        oc_feat_df = pd.DataFrame(oc_val)\n",
    "        ps_feat_df = pd.DataFrame(ps_val)\n",
    "\n",
    "        feat_df_path = os.path.join(repo_path, 'data', 'features', 'dataframes', folder)\n",
    "        if not os.path.exists(feat_df_path): os.makedirs(feat_df_path)\n",
    "\n",
    "        ft_feat_df.to_csv(os.path.join(feat_df_path, 'ft_block_features.csv'))\n",
    "        oc_feat_df.to_csv(os.path.join(feat_df_path, 'oc_block_features.csv'))\n",
    "        ps_feat_df.to_csv(os.path.join(feat_df_path, 'ps_block_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(sub, cond, cam, task, side, block):\n",
    "\n",
    "    read_scores = pd.read_excel(\n",
    "        os.path.join(\n",
    "        find_paths.find_onedrive_path('patientdata'),\n",
    "        f'scores_JB_JH_JR.xlsx'),\n",
    "        usecols='A:I'\n",
    "        )\n",
    "\n",
    "    read_scores.set_index('sub_cond_cam', inplace = True)\n",
    "\n",
    "    if side == 'left': side='lh'\n",
    "    elif side == 'right': side='rh'\n",
    "    \n",
    "    # read scores for all blocks of a subject in the same cond, cam per side\n",
    "    ext_scores = read_scores.loc[f'{sub}_{cond}_{cam}'][f'{task}_{side}']\n",
    "\n",
    "    if type(ext_scores) != float:\n",
    "        ls_int_sc = [int(s) for s in ext_scores if s in ['0', '1', '2', '3', '4']]\n",
    "        \n",
    "        if block == 'b1':\n",
    "            score = ls_int_sc[0]\n",
    "        elif block == 'b2':\n",
    "            score = ls_int_sc[1]\n",
    "        elif block == 'b3':\n",
    "            score = ls_int_sc[2]\n",
    "        else:\n",
    "            print(f'no scores for block {block} or block does not exist')\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ultraleap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2b0f49e3f9f86e05f3035101dad0b7b3d97321cf2c3e74a112da03721bd9215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
